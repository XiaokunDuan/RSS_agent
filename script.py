
# ==============================================================================
# ğŸ“š ä¿¡æ¯ç³»ç»Ÿ(IS)é¡¶åˆŠè®ºæ–‡æ‘˜è¦è‡ªåŠ¨æ±‡æ€»ç¨‹åº (MISQ & ISJ) - ä¿®å¤ç‰ˆ
# ä½œè€…: ChatGPT (GPT-5) & AI Assistant
# ç‰ˆæœ¬: 2.0
# æ›´æ–°æ—¥å¿—:
# - ä½¿ç”¨ feedparser æ›¿ä»£ RSSFeedLoader ä»¥è§£å†³ ISJ çš„ 403 Forbidden é”™è¯¯ã€‚
#   (This bypasses the loader's attempt to scrape the main article URL, which is blocked.)
# - åœ¨æ‘˜è¦ç”Ÿæˆå‰å¢åŠ å†…å®¹æ£€æŸ¥ï¼Œé¿å…ä¸ºç©ºçš„æ¡ç›®ï¼ˆå¦‚'Issue Information'ï¼‰æµªè´¹APIè°ƒç”¨ã€‚
# - å¢åŠ äº† MISQ çš„ RSS æºã€‚
# ==============================================================================

print("\n--> æ­£åœ¨åŠ è½½ä¸»ç¨‹åº...")

try:
    import os, csv, time, warnings
    from datetime import datetime, timedelta
    from collections import defaultdict
    warnings.filterwarnings("ignore", category=UserWarning)

    # NEW: Import feedparser and Document
    import feedparser
    from langchain_core.documents import Document

    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_classic.chains.summarize import load_summarize_chain
    from langchain_core.prompts import PromptTemplate
    import markdown

    # ==============================================================================
    # ğŸ”§ é…ç½® (Configuration)
    # ==============================================================================
    # âš ï¸ è¯·ä»ç¯å¢ƒå˜é‡ä¸­åŠ è½½æ‚¨çš„ Google API Key
    # ä¾‹å¦‚: export GOOGLE_API_KEY='YourActualApiKeyHere'
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    DRIVE_MEMORY_PATH = "." # Changed to local directory
    MEMORY_FILE = os.path.join(DRIVE_MEMORY_PATH, "processed_is_links2.csv")
    RETENTION_DAYS = 7 # è®°å¿†æ–‡ä»¶ä¿ç•™å¤©æ•°

    LLM_MODEL = "gemini-2.5-pro" # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
    LLM_TEMPERATURE = 0.2
    PROMPT_TEMPLATE = """Please analyze the following academic abstract in Chinese. From the text, extract the core research question, the methodology used, and the key findings. Present the output in a structured list format.

Original Abstract: "{text}"

---
- **ç ”ç©¶é—®é¢˜ (Research Question):**
- **ç ”ç©¶æ–¹æ³• (Methodology):**
- **ä¸»è¦å‘ç° (Key Findings):**
"""

    # ==============================================================================
    # ğŸ§© RSS æº (RSS Feeds)
    # ==============================================================================
    MEDIA_SOURCES = {
        "MIS Quarterly (MISQ)": "https://aisel.aisnet.org/misq/recent.rss",
        "Information Systems Journal (ISJ)": "https://onlinelibrary.wiley.com/feed/13652575/most-recent",
        "European Journal of Information Systems (EJIS)": "https://www.tandfonline.com/feed/rss/tjis20"
    }

    # ==============================================================================
    # âš™ï¸ åŠŸèƒ½å‡½æ•° (Core Functions) - v3.0 æ‰¹é‡å¤„ç†ç‰ˆ
    # ==============================================================================
    def load_processed_links(memory_file):
        processed = set()
        if not os.path.exists(memory_file):
            return processed
        try:
            with open(memory_file, 'r', newline='', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if row:
                        processed.add(row[0])
            print(f"\n--> ä» {memory_file} åŠ è½½äº† {len(processed)} æ¡å·²å¤„ç†é“¾æ¥ã€‚")
        except Exception as e:
            print(f"--> âš ï¸ è¯»å–è®°å¿†æ–‡ä»¶å¤±è´¥: {e}")
        return processed

    def fetch_articles(sources: dict, processed_links: set):
        print("\n--> æ­£åœ¨åŠ è½½å„æœŸåˆŠ RSS æº...")
        all_docs = []
        newly_processed_links = []

        for name, url in sources.items():
            print(f"    - æ­£åœ¨å¤„ç†ã€{name}ã€‘...")
            try:
                feed = feedparser.parse(url)
                if feed.bozo:
                    print(f"      âš ï¸ ã€{name}ã€‘çš„RSSæºå¯èƒ½æ ¼å¼ä¸æ­£ç¡®: {feed.bozo_exception}")

                docs_count = 0
                for entry in feed.entries:
                    link = entry.get("link", "N/A")
                    if link in processed_links or link == "N/A":
                        continue

                    content = entry.get("summary") or entry.get("dc_description") or entry.get("description", "")
                    # æ”¹è¿›å†…å®¹æ£€æŸ¥ï¼šè·³è¿‡ç©ºå†…å®¹æˆ–è¿‡çŸ­çš„å†…å®¹ï¼ˆå¯èƒ½ä¸æ˜¯æ‘˜è¦ï¼‰
                    MIN_CONTENT_LENGTH = 100
                    if not content or len(content) < MIN_CONTENT_LENGTH:
                        continue

                    doc = Document(
                        page_content=content,
                        metadata={
                            "link": link,
                            "title": entry.get("title", "N/A"),
                            "source_name": name
                        }
                    )
                    all_docs.append(doc)
                    newly_processed_links.append(link)
                    docs_count += 1

                if docs_count > 0:
                    print(f"      âœ… æ‰¾åˆ° {docs_count} ç¯‡æ–°æ–‡ç« ã€‚")
                else:
                    print("      - æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡ç« ã€‚")

            except Exception as e:
                print(f"      âŒ åŠ è½½æˆ–è§£æå¤±è´¥: {url}\n         é”™è¯¯: {str(e)[:120]}")

        print(f"\n--> å…±æ‰¾åˆ° {len(all_docs)} ç¯‡éœ€å¤„ç†çš„æ–°è®ºæ–‡ã€‚")
        return all_docs, newly_processed_links

    def analyze_articles_individually(docs, api_key, model, temp, prompt_str):
        print("\n--> æ­£åœ¨è°ƒç”¨ Gemini æ¨¡å‹é€ç¯‡åˆ†æè®ºæ–‡...")
        llm = ChatGoogleGenerativeAI(model=model, google_api_key=api_key, temperature=temp)
        prompt = PromptTemplate(template=prompt_str, input_variables=["text"])

        final_md = ""
        final_md += f"# å­¦æœ¯è®ºæ–‡åˆ†ææŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d')})\n\n"

        for i, doc in enumerate(docs):
            title = doc.metadata.get("title", "N/A")
            link = doc.metadata.get("link", "N/A")
            source_name = doc.metadata.get("source_name", "N/A")

            print(f"    - æ­£åœ¨åˆ†æ [{i+1}/{len(docs)}]ã€{source_name}ã€‘: {title[:50]}...")
            try:
                full_prompt = prompt.format(text=doc.page_content)
                result = llm.invoke(full_prompt)
                analysis = result.content if hasattr(result, 'content') else str(result)

                final_md += f"## {i+1}. {title}\n\n"
                final_md += f"**æ¥æº (Source):** {source_name}\n"
                final_md += f"**é“¾æ¥ (Link):** <{link}>\n\n"
                final_md += f"**æ‘˜è¦åˆ†æ:**\n{analysis}\n\n---\n\n"
                time.sleep(2)
            except Exception as e:
                print(f"      âŒ åˆ†ææ–‡ç« ã€Š{title[:50]}...ã€‹æ—¶å‡ºé”™: {e}")
                final_md += f"## {i+1}. {title}\n\n"
                final_md += f"**é“¾æ¥ (Link):** <{link}>\n\n"
                final_md += "**æ‘˜è¦åˆ†æ:**\nåˆ†æå¤±è´¥ã€‚\n\n---\n\n"
        return final_md



    def save_processed_links(memory_file, new_links):
        try:
            with open(memory_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                for link in new_links:
                    writer.writerow([link])
            print(f"\n--> {len(new_links)} ä¸ªæ–°é“¾æ¥å·²ä¿å­˜åˆ°: {memory_file}")
        except Exception as e:
            print(f"\n--> âŒ ä¿å­˜å¤„ç†è¿‡çš„é“¾æ¥å¤±è´¥: {e}")


    def save_report(report_md):
        report_file_path = "report.md"
        try:
            with open(report_file_path, "w", encoding="utf-8") as f:
                f.write(report_md)
            print(f"\n--> âœ… æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {report_file_path}")
        except Exception as e:
            print(f"\n--> âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    # ==============================================================================
    # ğŸš€ ä¸»æµç¨‹ (Main Workflow)
    # ==============================================================================
    def main():
        if not GOOGLE_API_KEY:
            print("âŒ é”™è¯¯: GOOGLE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚")
            print("è¯·åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½®è¯¥ç¯å¢ƒå˜é‡, ä¾‹å¦‚: export GOOGLE_API_KEY='YourActualApiKeyHere'")
            return

        processed_links = load_processed_links(MEMORY_FILE)
        all_docs, new_links = fetch_articles(MEDIA_SOURCES, processed_links)

        if all_docs:
            report = analyze_articles_individually(all_docs, GOOGLE_API_KEY, LLM_MODEL, LLM_TEMPERATURE, PROMPT_TEMPLATE)
            print("\n" + "="*50 + "\n âœ¨ ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ âœ¨\n" + "="*50)
            print(report)
            save_report(report)
            if new_links:
                save_processed_links(MEMORY_FILE, new_links)
        else:
            print("\nâœ… æœªä»ä»»ä½• RSS æºä¸­æ‰¾åˆ°å¯å¤„ç†çš„æ–°æ–‡ç« ï¼Œç¨‹åºæ‰§è¡Œå®Œæ¯•ã€‚")

    main()

except Exception as e:
    import traceback
    print(f"\nâŒ ç¨‹åºæ‰§è¡ŒæœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    traceback.print_exc()
